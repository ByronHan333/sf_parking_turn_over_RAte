{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-19T00:24:51.159840Z",
     "start_time": "2019-01-19T00:24:51.156679Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-19T00:24:55.340029Z",
     "start_time": "2019-01-19T00:24:55.337431Z"
    }
   },
   "outputs": [],
   "source": [
    "pyspark_submit_args = '--packages org.mongodb.spark:mongo-spark-connector_2.11:2.4.0 pyspark-shell'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = pyspark_submit_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-19T00:25:45.863587Z",
     "start_time": "2019-01-19T00:25:45.835487Z"
    }
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1276.load.\n: java.lang.ClassNotFoundException: Failed to find data source: com.mongodb.spark.sql.DefaultSource. Please find packages at http://spark.apache.org/third-party-projects.html\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:635)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:190)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:164)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.ClassNotFoundException: com.mongodb.spark.sql.DefaultSource.DefaultSource\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$23$$anonfun$apply$15.apply(DataSource.scala:618)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$23$$anonfun$apply$15.apply(DataSource.scala:618)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$23.apply(DataSource.scala:618)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$23.apply(DataSource.scala:618)\n\tat scala.util.Try.orElse(Try.scala:84)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:618)\n\t... 13 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-29728ac78845>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mdf_schedules\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark_schedules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"com.mongodb.spark.sql.DefaultSource\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mspark_transactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1286\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1276.load.\n: java.lang.ClassNotFoundException: Failed to find data source: com.mongodb.spark.sql.DefaultSource. Please find packages at http://spark.apache.org/third-party-projects.html\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:635)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:190)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:164)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.ClassNotFoundException: com.mongodb.spark.sql.DefaultSource.DefaultSource\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$23$$anonfun$apply$15.apply(DataSource.scala:618)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$23$$anonfun$apply$15.apply(DataSource.scala:618)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$23.apply(DataSource.scala:618)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$23.apply(DataSource.scala:618)\n\tat scala.util.Try.orElse(Try.scala:84)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:618)\n\t... 13 more\n"
     ]
    }
   ],
   "source": [
    "spark_meters = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"test_connection\") \\\n",
    "    .config(\"spark.mongodb.input.uri\", \"mongodb://ec2-18-191-205-220.us-east-2.compute.amazonaws.com/finalProject.meters\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# df_meters = spark_meters.read.format(\"com.mongodb.spark.sql.DefaultSource\").load()\n",
    "\n",
    "spark_schedules = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"test_connection\") \\\n",
    "    .config(\"spark.mongodb.input.uri\", \"mongodb://ec2-18-191-205-220.us-east-2.compute.amazonaws.com/finalProject.schedules\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# df_schedules = spark_schedules.read.format(\"com.mongodb.spark.sql.DefaultSource\").load()\n",
    "\n",
    "spark_transactions = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"test_connection\") \\\n",
    "    .config(\"spark.mongodb.input.uri\", \"mongodb://ec2-18-191-205-220.us-east-2.compute.amazonaws.com/finalProject.transactions\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "df_transactions = spark_transactions.read.format(\"com.mongodb.spark.sql.DefaultSource\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T22:47:13.119994Z",
     "start_time": "2019-01-18T22:47:08.061253Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "#Feel free to add other libraries from pyspark\n",
    "\n",
    "# conf = SparkConf().setAppName(app_name)\n",
    "sc = SparkContext.getOrCreate()\n",
    "sc.setLogLevel(\"OFF\")\n",
    "\n",
    "ss = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T22:49:06.380368Z",
     "start_time": "2019-01-18T22:49:06.359668Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.1.133.172:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T22:49:16.816858Z",
     "start_time": "2019-01-18T22:49:16.814553Z"
    }
   },
   "outputs": [],
   "source": [
    "Parking_meters = 'Parking_meters.csv'\n",
    "transcation_input = 'output1month.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T22:49:34.486354Z",
     "start_time": "2019-01-18T22:49:29.589059Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "+---------+-----+----------+-------+----------+----------+----------+----------+----------+------+----------+----------+----------+--------+----------+--------------------+\n",
      "|  POST_ID|MS_ID|MS_SPACEID|CAP_COL|METER_TYPE|SMART_METE|ACTIVESENS|JURISDICTI|ON_OFF_STR|OSP_ID|STREET_NUM|STREETNAME|STREET_SEG|RATEAREA|SFPARKAREA|            LOCATION|\n",
      "+---------+-----+----------+-------+----------+----------+----------+----------+----------+------+----------+----------+----------+--------+----------+--------------------+\n",
      "|401-06340|    -|         0|   Grey|        SS|         N|         N|     SFMTA|        ON|     0|       634|  ELLIS ST|   5177000|  Area 3|          |(37.78436, -122.4...|\n",
      "+---------+-----+----------+-------+----------+----------+----------+----------+----------+------+----------+----------+----------+--------+----------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([StructField(\"POST_ID\", StringType(), True), \n",
    "                    StructField(\"MS_ID\", StringType(), True),\n",
    "                    StructField(\"MS_SPACEID\", StringType(), True),\n",
    "                    StructField(\"CAP_COL\", StringType(), True), \n",
    "                    StructField(\"METER_TYPE\", StringType(), True),\n",
    "                    StructField(\"SMART_METE\", StringType(), True),\n",
    "                    StructField(\"ACTIVESENS\", StringType(), True),\n",
    "                    StructField(\"JURISDICTI\", StringType(), True),\n",
    "                    StructField(\"ON_OFF_STR\", StringType(), True),\n",
    "                    StructField(\"OSP_ID\", StringType(), True),\n",
    "                    StructField(\"STREET_NUM\", StringType(), True),\n",
    "                    StructField(\"STREETNAME\", StringType(), True),\n",
    "                    StructField(\"STREET_SEG\", StringType(), True),\n",
    "                    StructField(\"RATEAREA\", StringType(), True),\n",
    "                    StructField(\"SFPARKAREA\", StringType(), True),\n",
    "                    StructField(\"LOCATION\", StringType(), True)])\n",
    "\n",
    "parking_meters = ss.read.csv(Parking_meters, schema = schema,header=True)\n",
    "print(len(parking_meters.columns))\n",
    "parking_meters.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T22:49:39.718027Z",
     "start_time": "2019-01-18T22:49:39.419951Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+----------+-------+----------+----------+----------+----------+\n",
      "|  POST_ID|MS_ID|MS_SPACEID|CAP_COL|METER_TYPE|SMART_METE|ACTIVESENS|ON_OFF_STR|\n",
      "+---------+-----+----------+-------+----------+----------+----------+----------+\n",
      "|401-06340|    -|         0|   Grey|        SS|         N|         N|        ON|\n",
      "|104-03190|    -|         0|   Grey|        SS|         N|         Y|        ON|\n",
      "|352-04350|    -|         0|   Grey|        SS|         N|         N|        ON|\n",
      "+---------+-----+----------+-------+----------+----------+----------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parking_meters_selected = parking_meters.select(['POST_ID','MS_ID','MS_SPACEID','CAP_COL','METER_TYPE','SMART_METE','ACTIVESENS','ON_OFF_STR'])\n",
    "parking_meters_selected.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T22:49:56.037858Z",
     "start_time": "2019-01-18T22:49:55.173179Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22426\n",
      "+---------+-----+----------+-------+----------+----------+----------+----------+\n",
      "|  POST_ID|MS_ID|MS_SPACEID|CAP_COL|METER_TYPE|SMART_METE|ACTIVESENS|ON_OFF_STR|\n",
      "+---------+-----+----------+-------+----------+----------+----------+----------+\n",
      "|401-06340|    -|         0|   Grey|        SS|         N|         N|        ON|\n",
      "|104-03190|    -|         0|   Grey|        SS|         N|         Y|        ON|\n",
      "|352-04350|    -|         0|   Grey|        SS|         N|         N|        ON|\n",
      "|116-03980|    -|         0|   Grey|        SS|         N|         N|        ON|\n",
      "|224-27570|    -|         0|   Grey|        SS|         N|         N|        ON|\n",
      "+---------+-----+----------+-------+----------+----------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parking_meters_cars = parking_meters_selected.filter((parking_meters_selected.CAP_COL == 'Green') | (parking_meters_selected.CAP_COL == 'Grey'))\n",
    "print(parking_meters_cars.count())\n",
    "parking_meters_cars.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T22:50:07.598124Z",
     "start_time": "2019-01-18T22:50:07.421786Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+----------+----------+----------+----------+\n",
      "|  POST_ID|CAP_COL|METER_TYPE|SMART_METE|ACTIVESENS|ON_OFF_STR|\n",
      "+---------+-------+----------+----------+----------+----------+\n",
      "|401-06340|   Grey|        SS|         N|         N|        ON|\n",
      "|104-03190|   Grey|        SS|         N|         Y|        ON|\n",
      "|352-04350|   Grey|        SS|         N|         N|        ON|\n",
      "|116-03980|   Grey|        SS|         N|         N|        ON|\n",
      "|224-27570|   Grey|        SS|         N|         N|        ON|\n",
      "+---------+-------+----------+----------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parking_meters_tojoin = parking_meters_cars.select(['POST_ID','CAP_COL','METER_TYPE','SMART_METE','ACTIVESENS','ON_OFF_STR'])\n",
    "parking_meters_tojoin.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T22:50:30.969658Z",
     "start_time": "2019-01-18T22:50:28.424739Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "root\n",
      " |-- POST_ID: string (nullable = true)\n",
      " |-- STREET_BLOCK: string (nullable = true)\n",
      " |-- PAYMENT_TYPE: string (nullable = true)\n",
      " |-- SESSION_START_DT: string (nullable = true)\n",
      " |-- SESSION_END_DT: string (nullable = true)\n",
      " |-- METER_EVENT_TYPE: string (nullable = true)\n",
      " |-- GROSS_PAID_AMT: double (nullable = true)\n",
      "\n",
      "+---------+--------------+------------+--------------------+--------------------+----------------+--------------+\n",
      "|  POST_ID|  STREET_BLOCK|PAYMENT_TYPE|    SESSION_START_DT|      SESSION_END_DT|METER_EVENT_TYPE|GROSS_PAID_AMT|\n",
      "+---------+--------------+------------+--------------------+--------------------+----------------+--------------+\n",
      "|490-22190|IRVING ST 2200|        CASH|19-OCT-18 11.00.0...|19-OCT-18 11.09.2...|              NS|          0.35|\n",
      "+---------+--------------+------------+--------------------+--------------------+----------------+--------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([StructField(\"POST_ID\", StringType(), True), \n",
    "                    StructField(\"MS_ID\", StringType(), True),\n",
    "                    StructField(\"MS_SPACEID\", StringType(), True),\n",
    "                    StructField(\"CAP_COL\", StringType(), True), \n",
    "                    StructField(\"METER_TYPE\", StringType(), True),\n",
    "                    StructField(\"SMART_METE\", StringType(), True),\n",
    "                    StructField(\"ACTIVESENS\", StringType(), True),\n",
    "                    StructField(\"JURISDICTI\", StringType(), True),\n",
    "                    StructField(\"ON_OFF_STR\", StringType(), True),\n",
    "                    StructField(\"OSP_ID\", StringType(), True),\n",
    "                    StructField(\"STREET_NUM\", StringType(), True),\n",
    "                    StructField(\"STREETNAME\", StringType(), True),\n",
    "                    StructField(\"STREET_SEG\", StringType(), True),\n",
    "                    StructField(\"RATEAREA\", StringType(), True),\n",
    "                    StructField(\"SFPARKAREA\", StringType(), True),\n",
    "                    StructField(\"LOCATION\", StringType(), True)])\n",
    "\n",
    "trans = ss.read.csv(transcation_input,header=True, inferSchema='true')\n",
    "print(len(trans.columns))\n",
    "trans.printSchema()\n",
    "trans.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T22:51:23.465400Z",
     "start_time": "2019-01-18T22:51:23.461651Z"
    }
   },
   "outputs": [],
   "source": [
    "def toTimeSafe(inval):\n",
    "    try:\n",
    "        return datetime.strptime(inval, \"%d-%b-%y %I.%M.%S %p\")\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def toFloatSafe(inval):\n",
    "    try:\n",
    "        return float(inval)\n",
    "    except ValueError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T22:51:24.579693Z",
     "start_time": "2019-01-18T22:51:24.574707Z"
    }
   },
   "outputs": [],
   "source": [
    "def stringToPost(row):\n",
    "    r = row.split(\",\")\n",
    "    return Row(\n",
    "    r[0].lstrip('\\\"').rstrip('\\\"'),\\\n",
    "      r[1].lstrip('\\\"').rstrip('\\\"'),\\\n",
    "      r[2].lstrip('\\\"').rstrip('\\\"'),\\\n",
    "      toTimeSafe(r[3].lstrip('\\\"').rstrip('\\\"')),\\\n",
    "      toTimeSafe(r[4].lstrip('\\\"').rstrip('\\\"')),\\\n",
    "      r[5].lstrip('\\\"').rstrip('\\\"'),\\\n",
    "      toFloatSafe(r[6].lstrip('\\\"').rstrip('\\\"'))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T22:51:36.955245Z",
     "start_time": "2019-01-18T22:51:36.912020Z"
    }
   },
   "outputs": [],
   "source": [
    "trans = sc.textFile(transcation_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T22:51:45.509952Z",
     "start_time": "2019-01-18T22:51:44.966041Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['490-22190,IRVING ST 2200,CASH,19-OCT-18 11.00.01 AM,19-OCT-18 11.09.21 AM,NS,0.35',\n",
       " '823-00160,CHESTNUT ST 0,CASH,13-OCT-18 03.30.34 PM,13-OCT-18 04.33.25 PM,AT,0.05']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans = sc.textFile(transcation_input)\n",
    "tmp = trans.first()\n",
    "trans = trans.filter(lambda x: x!= tmp)\n",
    "trans.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T22:52:13.537787Z",
     "start_time": "2019-01-18T22:52:13.473637Z"
    }
   },
   "outputs": [],
   "source": [
    "trans = sc.textFile(transcation_input)\n",
    "tmp = trans.first()\n",
    "trans = trans.filter(lambda x: x!= tmp)\n",
    "tran_df = trans.map(lambda x: stringToPost(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T22:52:20.806088Z",
     "start_time": "2019-01-18T22:52:20.407139Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+------------+-------------------+-------------------+----------------+--------------+\n",
      "|  POST_ID|        STREET_BLOCK|PAYMENT_TYPE|   SESSION_START_DT|     SESSION_END_DT|METER_EVENT_TYPE|GROSS_PAID_AMT|\n",
      "+---------+--------------------+------------+-------------------+-------------------+----------------+--------------+\n",
      "|490-22190|      IRVING ST 2200|        CASH|2018-10-19 11:00:01|2018-10-19 11:09:21|              NS|          0.35|\n",
      "|823-00160|       CHESTNUT ST 0|        CASH|2018-10-13 15:30:34|2018-10-13 16:33:25|              AT|          0.05|\n",
      "|490-21250|      IRVING ST 2100|        CASH|2018-10-29 12:40:30|2018-10-29 13:07:10|              NS|           1.0|\n",
      "|440-37030|     GEARY BLVD 3700| CREDIT CARD|2018-10-30 13:36:13|2018-10-30 13:49:33|              NS|           0.5|\n",
      "|540-00100|         LAGUNA ST 0| PAY BY CELL|2018-10-05 09:42:00|2018-10-05 10:12:00|              AT|          1.12|\n",
      "|700-12080|    VALENCIA ST 1200| PAY BY CELL|2018-10-12 09:00:00|2018-10-12 10:35:00|              NS|          4.35|\n",
      "|923-00001|Claremont and Ull...| CREDIT CARD|2018-10-26 11:30:54|2018-10-26 12:37:54|              NS|           2.5|\n",
      "|355-02120|       CHURCH ST 200| CREDIT CARD|2018-10-25 09:12:41|2018-10-25 11:12:41|              NS|           4.5|\n",
      "|540-28230|      LAGUNA ST 2800| CREDIT CARD|2018-10-10 09:41:19|2018-10-10 10:46:19|              NS|          3.25|\n",
      "|326-04360|        BEACH ST 400|        CASH|2018-10-22 09:02:41|2018-10-22 12:00:25|              NS|          2.25|\n",
      "+---------+--------------------+------------+-------------------+-------------------+----------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "schema = StructType([ StructField(\"POST_ID\", StringType(), True),\n",
    "                      StructField(\"STREET_BLOCK\", StringType(), True),\n",
    "                      StructField(\"PAYMENT_TYPE\", StringType(), True),\n",
    "                      StructField(\"SESSION_START_DT\", TimestampType(), True),\n",
    "                      StructField(\"SESSION_END_DT\", TimestampType(), True),\n",
    "                      StructField(\"METER_EVENT_TYPE\", StringType(), True),\n",
    "                      StructField(\"GROSS_PAID_AMT\", DoubleType(), True),\n",
    "                    ])\n",
    "transaction_df = ss.createDataFrame(tran_df, schema)\n",
    "transaction_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T22:52:35.671520Z",
     "start_time": "2019-01-18T22:52:35.667624Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- POST_ID: string (nullable = true)\n",
      " |-- STREET_BLOCK: string (nullable = true)\n",
      " |-- PAYMENT_TYPE: string (nullable = true)\n",
      " |-- SESSION_START_DT: timestamp (nullable = true)\n",
      " |-- SESSION_END_DT: timestamp (nullable = true)\n",
      " |-- METER_EVENT_TYPE: string (nullable = true)\n",
      " |-- GROSS_PAID_AMT: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transaction_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T22:52:53.829710Z",
     "start_time": "2019-01-18T22:52:53.827189Z"
    }
   },
   "source": [
    "## aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T22:55:59.541673Z",
     "start_time": "2019-01-18T22:55:59.538996Z"
    }
   },
   "outputs": [],
   "source": [
    "revenue_df = transaction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T22:57:05.601147Z",
     "start_time": "2019-01-18T22:57:05.128066Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+------------+----------------+--------------+--------------+--------------+---+------------+--------+\n",
      "|  POST_ID|        STREET_BLOCK|PAYMENT_TYPE|METER_EVENT_TYPE|GROSS_PAID_AMT|starttime_date|starttime_hour|dow|endtime_hour|timeDiff|\n",
      "+---------+--------------------+------------+----------------+--------------+--------------+--------------+---+------------+--------+\n",
      "|490-22190|      IRVING ST 2200|        CASH|              NS|          0.35|    2018-10-19|            11|  5|          11|     560|\n",
      "|823-00160|       CHESTNUT ST 0|        CASH|              AT|          0.05|    2018-10-13|            15|  6|          16|    3771|\n",
      "|490-21250|      IRVING ST 2100|        CASH|              NS|           1.0|    2018-10-29|            12|  1|          13|    1600|\n",
      "|440-37030|     GEARY BLVD 3700| CREDIT CARD|              NS|           0.5|    2018-10-30|            13|  2|          13|     800|\n",
      "|540-00100|         LAGUNA ST 0| PAY BY CELL|              AT|          1.12|     2018-10-5|            09|  5|          10|    1800|\n",
      "|700-12080|    VALENCIA ST 1200| PAY BY CELL|              NS|          4.35|    2018-10-12|            09|  5|          10|    5700|\n",
      "|923-00001|Claremont and Ull...| CREDIT CARD|              NS|           2.5|    2018-10-26|            11|  5|          12|    4020|\n",
      "|355-02120|       CHURCH ST 200| CREDIT CARD|              NS|           4.5|    2018-10-25|            09|  4|          11|    7200|\n",
      "|540-28230|      LAGUNA ST 2800| CREDIT CARD|              NS|          3.25|    2018-10-10|            09|  3|          10|    3900|\n",
      "|326-04360|        BEACH ST 400|        CASH|              NS|          2.25|    2018-10-22|            09|  1|          12|   10664|\n",
      "|552-06120|  LONG BRIDGE ST 600| CREDIT CARD|              NS|          0.25|     2018-10-8|            09|  1|          09|     720|\n",
      "|203-06420|         03RD ST 600|        CASH|              NS|           1.6|    2018-10-26|            15|  5|          15|    2095|\n",
      "|671-00030|        SOUTH PARK 0| CREDIT CARD|              AT|          8.69|    2018-10-30|            13|  2|          16|   10748|\n",
      "|340-00433|        BLUXOME ST 0| CREDIT CARD|              NS|          5.58|    2018-10-11|            14|  4|          17|   11109|\n",
      "|385-00220|        DOLORES ST 0| CREDIT CARD|              NS|          2.21|    2018-10-16|            17|  2|          18|    3519|\n",
      "|419-02050|     FRANKLIN ST 200| CREDIT CARD|              NS|          4.75|    2018-10-23|            11|  2|          13|    5094|\n",
      "|444-13180|      GRANT AVE 1300|        CASH|              AT|          1.25|    2018-10-25|            16|  4|          18|    5286|\n",
      "|120-12970|       20TH AVE 1200|        CASH|              AT|          0.45|    2018-10-13|            11|  6|          12|    1679|\n",
      "|600-06130|     PACIFIC AVE 600|        CASH|              NS|           1.2|    2018-10-24|            12|  3|          12|    1330|\n",
      "|700-12140|    VALENCIA ST 1200| PAY BY CELL|              NS|           5.5|    2018-10-22|            09|  1|          11|    7200|\n",
      "+---------+--------------------+------------+----------------+--------------+--------------+--------------+---+------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import unix_timestamp, from_unixtime, date_format, col\n",
    "\n",
    "revenue_df = revenue_df.withColumn('starttime_date',date_format('SESSION_START_DT','yyyy-M-d'))\n",
    "revenue_df = revenue_df.withColumn('starttime_hour',date_format('SESSION_START_DT','HH').cast('string'))\n",
    "revenue_df = revenue_df.withColumn('dow',date_format('SESSION_START_DT','u').cast('string'))\n",
    "revenue_df = revenue_df.withColumn('endtime_hour',date_format('SESSION_END_DT','HH').cast('string'))\n",
    "revenue_df = revenue_df.withColumn('timeDiff', unix_timestamp('SESSION_END_DT')- unix_timestamp('SESSION_START_DT'))\n",
    "revenue_df.drop('SESSION_START_DT','SESSION_END_DT').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T23:01:45.746210Z",
     "start_time": "2019-01-18T23:01:45.632153Z"
    }
   },
   "outputs": [],
   "source": [
    "wanted_df = revenue_df.groupBy(\"STREET_BLOCK\",\"dow\",\"starttime_hour\")\\\n",
    "                      .agg(avg('GROSS_PAID_AMT').alias('avgRevenue'), \n",
    "                      count('starttime_hour').alias('numNewTrans'),\n",
    "                      avg('timeDiff').alias('avgDur'),\n",
    "                      count(when(col('METER_EVENT_TYPE')=='NS', True)).alias('numNS'),\n",
    "                      count(when(col('METER_EVENT_TYPE')=='AT', True)).alias('numAT'),\n",
    "                      count(when(col('PAYMENT_TYPE')=='CASH', True)).alias('numCASH'),\n",
    "                      count(when(col('PAYMENT_TYPE')=='CREDIT CARD', True)).alias('numCC'),\n",
    "                      count(when(col('PAYMENT_TYPE')=='PAY BY CELL', True)).alias('numPhone'),\n",
    "                      count(when(col('PAYMENT_TYPE')=='SMART CARD', True)).alias('numSmartCard'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T23:04:24.697186Z",
     "start_time": "2019-01-18T23:01:47.927937Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---+--------------+------------------+-----------+------------------+-----+-----+-------+-----+--------+------------+\n",
      "|    STREET_BLOCK|dow|starttime_hour|        avgRevenue|numNewTrans|            avgDur|numNS|numAT|numCASH|numCC|numPhone|numSmartCard|\n",
      "+----------------+---+--------------+------------------+-----------+------------------+-----+-----+-------+-----+--------+------------+\n",
      "|    BROADWAY 100|  3|            10|           3.66375|         32|            6322.5|   19|   13|      5|   22|       5|           0|\n",
      "|PRESIDIO AVE 500|  1|            12|1.3291666666666666|         12|2544.3333333333335|   10|    2|      5|    7|       0|           0|\n",
      "+----------------+---+--------------+------------------+-----------+------------------+-----+-----+-------+-----+--------+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wanted_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T23:04:48.059165Z",
     "start_time": "2019-01-18T23:04:48.039854Z"
    }
   },
   "outputs": [],
   "source": [
    "wanted_df_derv = revenue_df.groupBy(\"STREET_BLOCK\",\"dow\",\"endtime_hour\").count()\n",
    "wanted_df_derv = wanted_df_derv.withColumnRenamed(\"STREET_BLOCK\", 'block_derv')\n",
    "wanted_df_derv = wanted_df_derv.withColumnRenamed('dow', 'dow_derv')\n",
    "# wanted_df_derv.sort(\"dow\",\"endtime_hour\", ascending=[True, True]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T23:05:05.710676Z",
     "start_time": "2019-01-18T23:05:05.621021Z"
    }
   },
   "outputs": [],
   "source": [
    "wanted_df = wanted_df.join(wanted_df_derv, \n",
    "                           (wanted_df.STREET_BLOCK == wanted_df_derv.block_derv) &\n",
    "                           (wanted_df.dow == wanted_df_derv.dow_derv) &\n",
    "                           (wanted_df.starttime_hour == wanted_df_derv.endtime_hour)).drop('endtime_hour')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T23:05:11.076479Z",
     "start_time": "2019-01-18T23:05:11.056364Z"
    }
   },
   "outputs": [],
   "source": [
    "wanted_df = wanted_df.withColumnRenamed('count','numEndTrans')\n",
    "wanted_df = wanted_df.withColumn('turnover', wanted_df['numNewTrans'] - wanted_df['numEndTrans'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T23:17:49.799432Z",
     "start_time": "2019-01-18T23:07:15.686772Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---+--------------+------------------+-----------+------------------+-----+-----+-------+-----+--------+------------+-----------+--------+\n",
      "|STREET_BLOCK|dow|starttime_hour|        avgRevenue|numNewTrans|            avgDur|numNS|numAT|numCASH|numCC|numPhone|numSmartCard|numEndTrans|turnover|\n",
      "+------------+---+--------------+------------------+-----------+------------------+-----+-----+-------+-----+--------+------------+-----------+--------+\n",
      "|   01ST ST 0|  1|            13| 1.512857142857143|         14|1475.5714285714287|   11|    3|      6|    3|       4|           1|         19|      -5|\n",
      "|   01ST ST 0|  1|            07|             1.375|          6|1353.6666666666667|    6|    0|      2|    0|       0|           4|          2|       4|\n",
      "|   01ST ST 0|  1|            17|             0.725|          2|            1030.5|    1|    1|      2|    0|       0|           0|          2|       0|\n",
      "|   01ST ST 0|  1|            15|               1.0|          2|             960.0|    2|    0|      2|    0|       0|           0|          5|      -3|\n",
      "|   01ST ST 0|  1|            18|              0.25|          4|               0.0|    4|    0|      4|    0|       0|           0|          4|       0|\n",
      "|   01ST ST 0|  1|            09|2.4153846153846152|         13|2361.3846153846152|   12|    1|      4|    2|       3|           4|         13|       0|\n",
      "|   01ST ST 0|  1|            10|          2.665625|         16|            2677.5|   15|    1|      2|    3|       1|          10|         13|       3|\n",
      "|   01ST ST 0|  1|            12| 2.489090909090909|         11|2430.5454545454545|   10|    1|      2|    5|       1|           3|         10|       1|\n",
      "|   01ST ST 0|  1|            19|              0.25|          3|               0.0|    3|    0|      3|    0|       0|           0|          3|       0|\n",
      "|   01ST ST 0|  1|            11|1.8342857142857143|         14|            1878.0|   12|    2|      3|    4|       0|           7|         15|      -1|\n",
      "|   01ST ST 0|  1|            14|1.3608333333333331|         12|1510.6666666666667|   11|    1|      6|    5|       0|           1|         13|      -1|\n",
      "|   01ST ST 0|  1|            08|              2.68|          8|           2639.75|    7|    1|      0|    3|       1|           4|          6|       2|\n",
      "|   01ST ST 0|  2|            15|             1.125|          2|            1080.0|    2|    0|      1|    0|       0|           1|          4|      -2|\n",
      "|   01ST ST 0|  2|            08|            2.9375|         12|2836.5833333333335|   11|    1|      2|    7|       0|           3|         14|      -2|\n",
      "|   01ST ST 0|  2|            12|2.1340909090909093|         22|2175.7727272727275|   20|    2|      3|    8|       1|          10|         26|      -4|\n",
      "|   01ST ST 0|  2|            16|              0.25|          2|               0.0|    2|    0|      2|    0|       0|           0|          2|       0|\n",
      "|   01ST ST 0|  2|            14|             1.782|         10|            1733.3|    9|    1|      4|    2|       2|           2|         19|      -9|\n",
      "|   01ST ST 0|  2|            10| 1.994814814814815|         27| 2153.222222222222|   25|    2|      8|    9|       1|           9|         26|       1|\n",
      "|   01ST ST 0|  2|            09|2.2771999999999997|         25|            2413.0|   17|    8|      0|   17|       5|           3|         19|       6|\n",
      "|   01ST ST 0|  2|            13|2.1626666666666665|         15|            2261.8|   14|    1|      4|    5|       4|           2|         18|      -3|\n",
      "+------------+---+--------------+------------------+-----------+------------------+-----+-----+-------+-----+--------+------------+-----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wanted_df = wanted_df.drop('block_derv','dow_derv')\n",
    "wanted_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T23:18:19.824619Z",
     "start_time": "2019-01-18T23:18:19.594887Z"
    }
   },
   "outputs": [],
   "source": [
    "schedules = ss.read.csv('Meter_Rate_Schedules.csv', header=True).withColumnRenamed(\"Post ID\", \"POST_ID\")\n",
    "meters = ss.read.csv('Parking_meters.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T23:18:21.268937Z",
     "start_time": "2019-01-18T23:18:21.254784Z"
    }
   },
   "outputs": [],
   "source": [
    "# Change all column names to uppercase\n",
    "for col in schedules.columns:\n",
    "    schedules = schedules.withColumnRenamed(col, \"_\".join(col.split()).upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T23:22:58.956361Z",
     "start_time": "2019-01-18T23:20:17.899460Z"
    }
   },
   "outputs": [],
   "source": [
    "# schedules.write.saveAsTable(\"Schedules\")\n",
    "transaction_df.select(['STREET_BLOCK', 'POST_ID']).distinct().write.saveAsTable(\"Transactions\")\n",
    "meters.write.saveAsTable(\"Meters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T23:23:18.694803Z",
     "start_time": "2019-01-18T23:23:16.279122Z"
    }
   },
   "outputs": [],
   "source": [
    "sched_base = ss.sql(\"\"\"\n",
    "SELECT POST_ID, AVG_RATE AS RATE, HAS_OVERRIDE FROM (\n",
    "\n",
    "    SELECT POST_ID, AVG_RATE, CASE WHEN num >= 2 THEN 1 ELSE 0 END HAS_OVERRIDE FROM\n",
    "    (\n",
    "        SELECT *,\n",
    "        count(1) OVER(\n",
    "        PARTITION BY POST_ID\n",
    "        ) num,\n",
    "        AVG(RATE) OVER(\n",
    "        PARTITION BY POST_ID\n",
    "        ) AVG_RATE\n",
    "\n",
    "        FROM Schedules\n",
    "    )    \n",
    ")\n",
    "GROUP BY 1, 2, 3\n",
    "\"\"\")\n",
    "sched_base.write.saveAsTable('Schedules_Base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T23:23:23.166990Z",
     "start_time": "2019-01-18T23:23:23.087001Z"
    }
   },
   "outputs": [],
   "source": [
    "meter_dummy = ss.sql(\"\"\"\n",
    "SELECT POST_ID,\n",
    "RATE,\n",
    "HAS_OVERRIDE,\n",
    "CASE WHEN CAP_COLOR = 'Green' THEN 1 ELSE 0 END GREEN,\n",
    "CASE WHEN CAP_COLOR = 'Grey' THEN 1 ELSE 0 END GREY\n",
    "FROM\n",
    "Schedules_Base\n",
    "join\n",
    "(\n",
    "    SELECT * FROM Meters\n",
    "    WHERE (Meters.CAP_COLOR = 'Green' OR Meters.CAP_COLOR = 'Grey')\n",
    ")\n",
    "USING(POST_ID)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T23:23:24.247853Z",
     "start_time": "2019-01-18T23:23:24.204042Z"
    }
   },
   "outputs": [],
   "source": [
    "block_post = ss.sql(\"\"\"\n",
    "    SELECT STREET_BLOCK, POST_ID\n",
    "    FROM Transactions\n",
    "    GROUP BY 1,2\n",
    "    ORDER BY 1\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T23:23:25.367756Z",
     "start_time": "2019-01-18T23:23:25.351046Z"
    }
   },
   "outputs": [],
   "source": [
    "block_post_full = block_post.join(meter_dummy, how='inner', on='POST_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T23:23:31.023238Z",
     "start_time": "2019-01-18T23:23:26.515470Z"
    }
   },
   "outputs": [],
   "source": [
    "block_post_full.write.saveAsTable(\"Post_block\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T23:23:32.113490Z",
     "start_time": "2019-01-18T23:23:32.098754Z"
    }
   },
   "outputs": [],
   "source": [
    "block_info = ss.sql(\"\"\"\n",
    "SELECT STREET_BLOCK,\n",
    "MAX(RATE) RATE,\n",
    "SUM(HAS_OVERRIDE) NUM_OR,\n",
    "SUM(GREEN) NUM_GREEN,\n",
    "SUM(GREY) NUM_GREY\n",
    "FROM Post_block\n",
    "GROUP BY 1\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T23:23:34.386418Z",
     "start_time": "2019-01-18T23:23:33.964298Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+------+---------+--------+\n",
      "|    STREET_BLOCK|              RATE|NUM_OR|NUM_GREEN|NUM_GREY|\n",
      "+----------------+------------------+------+---------+--------+\n",
      "|    16TH ST 2900|               2.0|     0|        2|      17|\n",
      "|  HAIGHT ST 1600|               2.0|     0|        1|      23|\n",
      "|COLUMBUS AVE 800|               2.0|     0|        2|       9|\n",
      "|       KERN ST 0|               2.0|     0|        0|       3|\n",
      "|     KING ST 300|1.1666666666666667|     4|        0|       4|\n",
      "|  HOWARD ST 1400|               2.0|     0|        4|      17|\n",
      "|     03RD ST 100|               3.5|     0|        0|      15|\n",
      "| GEARY BLVD 3800|               2.0|     0|        0|      13|\n",
      "|    JONES ST 800|               3.0|     0|        1|      18|\n",
      "|   24TH AVE 2500|               2.0|     0|        0|       4|\n",
      "|    GROVE ST 100|            1.6875|     6|        2|       4|\n",
      "|    03RD ST 4600|               2.0|     0|        0|       8|\n",
      "|     KING ST 100|             3.125|     7|        0|       7|\n",
      "|    10TH AVE 300|               2.0|     0|        2|      10|\n",
      "|    BEACH ST 500|              3.75|     3|        0|       3|\n",
      "|     08TH ST 600|               2.0|     0|       10|       8|\n",
      "|COLUMBUS AVE 400|               2.0|     0|        5|       7|\n",
      "|   BRYANT ST 800|               2.0|     0|       19|       0|\n",
      "|VALENCIA ST 1400|               2.0|     0|        0|      29|\n",
      "|    KIRKHAM ST 0|               2.0|     0|        0|      31|\n",
      "+----------------+------------------+------+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "block_info.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T23:23:36.467029Z",
     "start_time": "2019-01-18T23:23:36.318386Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "va = VectorAssembler(outputCol=\"features\", inputCols=block_info.columns[1:]) #except the last col.\n",
    "block_feat = va.transform(block_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T23:23:41.585651Z",
     "start_time": "2019-01-18T23:23:37.421721Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "kmeans =  KMeans(k = 90, maxIter = 200, tol = 0.1) # k = 10 as there are 10 different handwritten numbers.\n",
    "model = kmeans.fit(block_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T23:23:56.483075Z",
     "start_time": "2019-01-18T23:23:56.425341Z"
    }
   },
   "outputs": [],
   "source": [
    "kmeans_df = model.transform(block_feat).select('STREET_BLOCK', 'RATE', 'NUM_OR', 'NUM_GREEN', 'NUM_GREY', 'PREDICTION')\\\n",
    "    .withColumnRenamed('PREDICTION', 'CLUSTER')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T23:23:57.397077Z",
     "start_time": "2019-01-18T23:23:57.011224Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+------+---------+--------+-------+\n",
      "|    STREET_BLOCK|              RATE|NUM_OR|NUM_GREEN|NUM_GREY|CLUSTER|\n",
      "+----------------+------------------+------+---------+--------+-------+\n",
      "|    16TH ST 2900|               2.0|     0|        2|      17|     52|\n",
      "|  HAIGHT ST 1600|               2.0|     0|        1|      23|     53|\n",
      "|COLUMBUS AVE 800|               2.0|     0|        2|       9|     14|\n",
      "|       KERN ST 0|               2.0|     0|        0|       3|     61|\n",
      "|     KING ST 300|1.1666666666666667|     4|        0|       4|     42|\n",
      "+----------------+------------------+------+---------+--------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kmeans_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T23:24:01.340111Z",
     "start_time": "2019-01-18T23:24:01.299491Z"
    }
   },
   "outputs": [],
   "source": [
    "final_df = wanted_df.join(kmeans_df, \"STREET_BLOCK\", \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T23:29:34.804919Z",
     "start_time": "2019-01-18T23:24:02.243234Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---+--------------+------------------+-----------+------------------+-----+-----+-------+-----+--------+------------+-----------+--------+----+------+---------+--------+-------+\n",
      "|STREET_BLOCK|dow|starttime_hour|        avgRevenue|numNewTrans|            avgDur|numNS|numAT|numCASH|numCC|numPhone|numSmartCard|numEndTrans|turnover|RATE|NUM_OR|NUM_GREEN|NUM_GREY|CLUSTER|\n",
      "+------------+---+--------------+------------------+-----------+------------------+-----+-----+-------+-----+--------+------------+-----------+--------+----+------+---------+--------+-------+\n",
      "|   01ST ST 0|  1|            10|          2.665625|         16|            2677.5|   15|    1|      2|    3|       1|          10|         13|       3| 3.5|     0|        0|       2|     47|\n",
      "|   01ST ST 0|  1|            18|              0.25|          4|               0.0|    4|    0|      4|    0|       0|           0|          4|       0| 3.5|     0|        0|       2|     47|\n",
      "|   01ST ST 0|  1|            14|1.3608333333333331|         12|1510.6666666666667|   11|    1|      6|    5|       0|           1|         13|      -1| 3.5|     0|        0|       2|     47|\n",
      "|   01ST ST 0|  1|            12| 2.489090909090909|         11|2430.5454545454545|   10|    1|      2|    5|       1|           3|         10|       1| 3.5|     0|        0|       2|     47|\n",
      "|   01ST ST 0|  1|            17|             0.725|          2|            1030.5|    1|    1|      2|    0|       0|           0|          2|       0| 3.5|     0|        0|       2|     47|\n",
      "+------------+---+--------------+------------------+-----------+------------------+-----+-----+-------+-----+--------+------------+-----------+--------+----+------+---------+--------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.select(\"*\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T23:33:52.795746Z",
     "start_time": "2019-01-18T23:33:52.787780Z"
    }
   },
   "outputs": [],
   "source": [
    "finalal_df = final_df.drop(\"STREET_BLOCK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T23:39:17.177298Z",
     "start_time": "2019-01-18T23:33:53.475392Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+------------------+-----------+------------------+-----+-----+-------+-----+--------+------------+-----------+--------+----+------+---------+--------+-------+\n",
      "|dow|starttime_hour|        avgRevenue|numNewTrans|            avgDur|numNS|numAT|numCASH|numCC|numPhone|numSmartCard|numEndTrans|turnover|RATE|NUM_OR|NUM_GREEN|NUM_GREY|CLUSTER|\n",
      "+---+--------------+------------------+-----------+------------------+-----+-----+-------+-----+--------+------------+-----------+--------+----+------+---------+--------+-------+\n",
      "|  1|            10|          2.665625|         16|            2677.5|   15|    1|      2|    3|       1|          10|         13|       3| 3.5|     0|        0|       2|     47|\n",
      "|  1|            18|              0.25|          4|               0.0|    4|    0|      4|    0|       0|           0|          4|       0| 3.5|     0|        0|       2|     47|\n",
      "|  1|            14|1.3608333333333331|         12|1510.6666666666667|   11|    1|      6|    5|       0|           1|         13|      -1| 3.5|     0|        0|       2|     47|\n",
      "|  1|            12| 2.489090909090909|         11|2430.5454545454545|   10|    1|      2|    5|       1|           3|         10|       1| 3.5|     0|        0|       2|     47|\n",
      "|  1|            17|             0.725|          2|            1030.5|    1|    1|      2|    0|       0|           0|          2|       0| 3.5|     0|        0|       2|     47|\n",
      "+---+--------------+------------------+-----------+------------------+-----+-----+-------+-----+--------+------------+-----------+--------+----+------+---------+--------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finalal_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T23:52:08.618654Z",
     "start_time": "2019-01-18T23:52:08.615015Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dow: string (nullable = true)\n",
      " |-- starttime_hour: string (nullable = true)\n",
      " |-- avgRevenue: double (nullable = true)\n",
      " |-- numNewTrans: long (nullable = false)\n",
      " |-- avgDur: double (nullable = true)\n",
      " |-- numNS: long (nullable = false)\n",
      " |-- numAT: long (nullable = false)\n",
      " |-- numCASH: long (nullable = false)\n",
      " |-- numCC: long (nullable = false)\n",
      " |-- numPhone: long (nullable = false)\n",
      " |-- numSmartCard: long (nullable = false)\n",
      " |-- numEndTrans: long (nullable = false)\n",
      " |-- turnover: long (nullable = false)\n",
      " |-- RATE: double (nullable = true)\n",
      " |-- NUM_OR: long (nullable = true)\n",
      " |-- NUM_GREEN: long (nullable = true)\n",
      " |-- NUM_GREY: long (nullable = true)\n",
      " |-- CLUSTER: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finalal_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-19T00:46:03.385801Z",
     "start_time": "2019-01-19T00:29:49.894719Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "def indexStringColumns(df, cols):\n",
    "    #variable newdf will be updated several times\n",
    "    newdf = df\n",
    "    \n",
    "    for c in cols:\n",
    "        #For each given colum, fits StringIndexerModel.\n",
    "        si = StringIndexer(inputCol=c, outputCol=c+\"-num\",stringOrderType=\"alphabetDesc\")\n",
    "        sm = si.fit(newdf)\n",
    "        #Creates a DataFame by putting the transformed values in the new colum with suffix \"-num\" \n",
    "        #and then drops the original columns.\n",
    "        #and drop the \"-num\" suffix. \n",
    "        newdf = sm.transform(newdf).drop(c)\n",
    "        newdf = newdf.withColumnRenamed(c+\"-num\", c)\n",
    "    return newdf\n",
    "\n",
    "stringindex_anotherdf = indexStringColumns(finalal_df, ['CLUSTER','dow','starttime_hour'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-19T00:46:09.968747Z",
     "start_time": "2019-01-19T00:46:09.909603Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "def oneHotEncodeColumns(df, cols):\n",
    "    newdf = df\n",
    "    for c in cols:\n",
    "        #For each given colum, create OneHotEncoder. \n",
    "        #dropLast : Whether to drop the last category in the encoded vector (default: true)\n",
    "        onehotenc = OneHotEncoder(inputCol=c, outputCol=c+\"-onehot\", dropLast=False)\n",
    "        #Creates a DataFame by putting the transformed values in the new colum with suffix \"-onehot\" \n",
    "        #and then drops the original columns.\n",
    "        #and drop the \"-onehot\" suffix. \n",
    "        newdf = onehotenc.transform(newdf).drop(c)\n",
    "        newdf = newdf.withColumnRenamed(c+\"-onehot\", c)\n",
    "    return newdf\n",
    "\n",
    "onehot_anotherdf = oneHotEncodeColumns(stringindex_anotherdf, ['CLUSTER','dow','starttime_hour'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-19T00:51:40.574106Z",
     "start_time": "2019-01-19T00:46:16.959280Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+------+-----+-----+-------+-----+--------+------------+-----------+--------+----+------+---------+--------+---------------+-------------+---------------+\n",
      "|avgRevenue|numNewTrans|avgDur|numNS|numAT|numCASH|numCC|numPhone|numSmartCard|numEndTrans|turnover|RATE|NUM_OR|NUM_GREEN|NUM_GREY|        CLUSTER|          dow| starttime_hour|\n",
      "+----------+-----------+------+-----+-----+-------+-----+--------+------------+-----------+--------+----+------+---------+--------+---------------+-------------+---------------+\n",
      "|  2.665625|         16|2677.5|   15|    1|      2|    3|       1|          10|         13|       3| 3.5|     0|        0|       2|(90,[47],[1.0])|(7,[6],[1.0])|(24,[13],[1.0])|\n",
      "+----------+-----------+------+-----+-----+-------+-----+--------+------------+-----------+--------+----+------+---------+--------+---------------+-------------+---------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "onehot_anotherdf.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-19T00:52:25.544711Z",
     "start_time": "2019-01-19T00:52:25.541139Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- avgRevenue: double (nullable = true)\n",
      " |-- numNewTrans: long (nullable = false)\n",
      " |-- avgDur: double (nullable = true)\n",
      " |-- numNS: long (nullable = false)\n",
      " |-- numAT: long (nullable = false)\n",
      " |-- numCASH: long (nullable = false)\n",
      " |-- numCC: long (nullable = false)\n",
      " |-- numPhone: long (nullable = false)\n",
      " |-- numSmartCard: long (nullable = false)\n",
      " |-- numEndTrans: long (nullable = false)\n",
      " |-- turnover: long (nullable = false)\n",
      " |-- RATE: double (nullable = true)\n",
      " |-- NUM_OR: long (nullable = true)\n",
      " |-- NUM_GREEN: long (nullable = true)\n",
      " |-- NUM_GREY: long (nullable = true)\n",
      " |-- CLUSTER: vector (nullable = true)\n",
      " |-- dow: vector (nullable = true)\n",
      " |-- starttime_hour: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "onehot_anotherdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-19T00:57:43.570822Z",
     "start_time": "2019-01-19T00:52:29.604041Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|            features|turnover|\n",
      "+--------------------+--------+\n",
      "|(104,[0,1,2,3,4,5...|       3|\n",
      "|(104,[0,1,3,5,9,1...|       0|\n",
      "|(104,[0,1,2,3,4,5...|      -1|\n",
      "+--------------------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Merging the data with Vector Assembler.\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"avgRevenue\",\"numNewTrans\",\"avgDur\",\"numNS\",\"numAT\",\"numCASH\",\"numCC\",\"numPhone\",\"numSmartCard\",\"numEndTrans\",\"RATE\",\"NUM_OR\",\"NUM_GREEN\",\"NUM_GREY\",\"CLUSTER\"],\n",
    "    outputCol=\"features\")\n",
    "\n",
    "output = assembler.transform(onehot_anotherdf)\n",
    "\n",
    "# va = VectorAssembler(outputCol=\"features\", inputCols=anotherdf.columns[1:]) #except the last col.\n",
    "penlpoints = output.select(\"features\", \"turnover\")\n",
    "penlpoints.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-19T01:08:46.468308Z",
     "start_time": "2019-01-19T00:58:39.911298Z"
    }
   },
   "outputs": [],
   "source": [
    "splits = penlpoints.randomSplit([0.8, 0.2])\n",
    "\n",
    "#cache() : the algorithm is interative and training and data sets are going to be reused many times.\n",
    "train = splits[0].cache()\n",
    "test = splits[1].cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-19T01:14:59.852611Z",
     "start_time": "2019-01-19T01:14:50.919598Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train the data.\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "# Paramenters\n",
    "#maxDepth : maximum tree depth (default : 5).\n",
    "#maxBins : maximum number of bins when binning continuous features (default : 32).\n",
    "#minInstancesPerNode : minimum number of dataset samples each branch needs to have after a split (default : 1).\n",
    "#minInfoGain : minimum information gain for a split (default : 0).\n",
    "dt = DecisionTreeRegressor(labelCol=\"turnover\")\n",
    "dtmodel = dt.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-19T01:15:02.269591Z",
     "start_time": "2019-01-19T01:15:02.223943Z"
    }
   },
   "outputs": [],
   "source": [
    "#Test data.\n",
    "dtpredicts = dtmodel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-19T01:15:08.943942Z",
     "start_time": "2019-01-19T01:15:03.165901Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.865871510197337\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator = RegressionEvaluator(labelCol=\"turnover\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(dtpredicts)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-19T01:26:56.653483Z",
     "start_time": "2019-01-19T01:26:45.650506Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.555589808990208\n"
     ]
    }
   ],
   "source": [
    "# Train the data.\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Paramenters\n",
    "#maxDepth : maximum tree depth (default : 5).\n",
    "#maxBins : maximum number of bins when binning continuous features (default : 32).\n",
    "#minInstancesPerNode : minimum number of dataset samples each branch needs to have after a split (default : 1).\n",
    "#minInfoGain : minimum information gain for a split (default : 0).\n",
    "dt = RandomForestRegressor(labelCol=\"turnover\")\n",
    "dtmodel = dt.fit(train)\n",
    "#Test data.\n",
    "dtpredicts = dtmodel.transform(test)\n",
    "evaluator = RegressionEvaluator(labelCol=\"turnover\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(dtpredicts)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
