{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark_submit_args = '--packages org.mongodb.spark:mongo-spark-connector_2.11:2.4.0 pyspark-shell'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = pyspark_submit_args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_meters = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"test_connection\") \\\n",
    "    .config(\"spark.mongodb.input.uri\", \"mongodb://ec2-18-191-205-220.us-east-2.compute.amazonaws.com/finalProject.meters\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "df_meters = spark_meters.read.format(\"com.mongodb.spark.sql.DefaultSource\").load()\n",
    "\n",
    "spark_schedules = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"test_connection\") \\\n",
    "    .config(\"spark.mongodb.input.uri\", \"mongodb://ec2-18-191-205-220.us-east-2.compute.amazonaws.com/finalProject.schedules\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "df_schedules = spark_schedules.read.format(\"com.mongodb.spark.sql.DefaultSource\").load()\n",
    "\n",
    "spark_transactions = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"test_connection\") \\\n",
    "    .config(\"spark.mongodb.input.uri\", \"mongodb://ec2-18-191-205-220.us-east-2.compute.amazonaws.com/finalProject.transactions\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "df_transactions = spark_transactions.read.format(\"com.mongodb.spark.sql.DefaultSource\").load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration of `DF`s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of observations in each `df`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "df_meters: 28997\n",
    "df_schedules: 60485\n",
    "df_transactions: 42263968\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[GROSS_PAID_AMT: double, METER_EVENT_TYPE: string, PAYMENT_TYPE: string, POST_ID: string, SESSION_END_DT: string, SESSION_START_DT: string, STREET_BLOCK: string, _id: struct<oid:string>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_meters.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "df_schedules.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "df_transactions.persist(StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+\n",
      "|PAYMENT_TYPE|  count|\n",
      "+------------+-------+\n",
      "|  SMART CARD|1824697|\n",
      "| PAY BY CELL|4633609|\n",
      "+------------+-------+\n",
      "only showing top 2 rows\n",
      "\n",
      "Elapsed: 205.944715977 seconds.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "df_transactions.distinct() \\\n",
    "               .groupBy('PAYMENT_TYPE') \\\n",
    "               .count() \\\n",
    "               .orderBy(['count', 'PAYMENT_TYPE'], ascending=[True, False]) \\\n",
    "               .show(2)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed = end_time - start_time\n",
    "print \"Elapsed: \" + str(elapsed) + \" seconds.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`df_meters`**\n",
    "- Trial 1 (no cacheing) \n",
    "    - `4.386 seconds`\n",
    "- Trial 2 (cached)\n",
    "    - `5.76 seconds`\n",
    "    - `1.21 seconds`\n",
    "**`df_transactions`**\n",
    "- Trial 1 (no cacheing)\n",
    "    - `175 seconds`\n",
    "- Trial 2 (StorageLevel.MEMORY_AND_DISK)\n",
    "    - `185.9 seconds`\n",
    "    - `1.40 seconds`\n",
    "- Trial 3 (different action -- `.distinct()` then `.count()`. Before, just `count()`)\n",
    "    - `24 seconds`\n",
    "    - `16.79 seconds`\n",
    "- Trial 4 (Trial 3, `.distinct()` then `show()`)\n",
    "    - `10 seconds`\n",
    "- Trial 5 (`.distinct()` then `.orderBy(['POST_ID', 'SESSION_END_DT'], ascending=[True, False]) \\` -- an intensive process\n",
    "    - `16 seconds`\n",
    "    - `15 seconds`\n",
    "- Trial 6 (`.distinct()`, `.groupBy()`, `.count()`, `.orderBy()`, `.show(2)`\n",
    "    - `16 seconds`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ACTIVESENS: string, CAP_COLOR: string, JURISDICTI: string, LOCATION: string, METER_TYPE: string, MS_ID: string, MS_SPACEID: int, ON_OFF_STR: string, OSP_ID: int, POST_ID: string, RATEAREA: string, SFPARKAREA: string, SMART_METE: string, STREETNAME: string, STREET_NUM: int, STREET_SEG: int, _id: struct<oid:string>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_meters.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "#Feel free to add other libraries from pyspark\n",
    "\n",
    "# conf = SparkConf().setAppName(app_name)\n",
    "sc = SparkContext.getOrCreate()\n",
    "sc.setLogLevel(\"OFF\")\n",
    "\n",
    "ss = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toTimeSafe(inval):\n",
    "    try:\n",
    "        return datetime.strptime(inval, \"%d-%b-%y %I.%M.%S %p\")\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def toFloatSafe(inval):\n",
    "    try:\n",
    "        return float(inval)\n",
    "    except ValueError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- GROSS_PAID_AMT: double (nullable = true)\n",
      " |-- METER_EVENT_TYPE: string (nullable = true)\n",
      " |-- PAYMENT_TYPE: string (nullable = true)\n",
      " |-- POST_ID: string (nullable = true)\n",
      " |-- SESSION_END_DT: string (nullable = true)\n",
      " |-- SESSION_START_DT: string (nullable = true)\n",
      " |-- STREET_BLOCK: string (nullable = true)\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transactions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+------------+---------+--------------------+--------------------+---------------+--------------------+\n",
      "|GROSS_PAID_AMT|METER_EVENT_TYPE|PAYMENT_TYPE|  POST_ID|      SESSION_END_DT|    SESSION_START_DT|   STREET_BLOCK|                 _id|\n",
      "+--------------+----------------+------------+---------+--------------------+--------------------+---------------+--------------------+\n",
      "|          0.35|              NS|        CASH|568-22390|02-MAR-18 10.17.4...|02-MAR-18 10.05.4...|MISSION ST 2200|[5c40f9e517f951ee...|\n",
      "|           0.1|              NS|        CASH|680-17110|26-FEB-18 05.18.0...|26-FEB-18 05.15.2...|TARAVAL ST 1700|[5c40f9e517f951ee...|\n",
      "|          1.25|              NS|  SMART CARD|360-05240|15-FEB-18 09.51.5...|15-FEB-18 09.18.3...| CLEMENT ST 500|[5c40f9e517f951ee...|\n",
      "|          3.75|              NS| CREDIT CARD|321-03100|06-JUN-18 12.59.5...|06-JUN-18 11.07.2...|  BALBOA ST 300|[5c40f9e517f951ee...|\n",
      "|          1.12|              AT| PAY BY CELL|462-05360|20-JAN-18 02.20.4...|20-JAN-18 01.50.4...|  HAIGHT ST 500|[5c40f9e517f951ee...|\n",
      "+--------------+----------------+------------+---------+--------------------+--------------------+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transactions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql.functions import udf\n",
    "func =  udf (lambda x: datetime.strptime(x.lower(), '%d-%b-%y %H.%M.%S %p'), TimestampType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transactions = df_transactions.withColumn('SESSION_START_DT', func(col('SESSION_START_DT')))\n",
    "df_transactions = df_transactions.withColumn('SESSION_END_DT', func(col('SESSION_END_DT')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+------------+---------+-------------------+-------------------+---------------+\n",
      "|GROSS_PAID_AMT|METER_EVENT_TYPE|PAYMENT_TYPE|  POST_ID|     SESSION_END_DT|   SESSION_START_DT|   STREET_BLOCK|\n",
      "+--------------+----------------+------------+---------+-------------------+-------------------+---------------+\n",
      "|          0.35|              NS|        CASH|568-22390|2018-03-02 10:17:46|2018-03-02 10:05:46|MISSION ST 2200|\n",
      "|           0.1|              NS|        CASH|680-17110|2018-02-26 05:18:08|2018-02-26 05:15:28|TARAVAL ST 1700|\n",
      "|          1.25|              NS|  SMART CARD|360-05240|2018-02-15 09:51:57|2018-02-15 09:18:37| CLEMENT ST 500|\n",
      "|          3.75|              NS| CREDIT CARD|321-03100|2018-06-06 12:59:52|2018-06-06 11:07:22|  BALBOA ST 300|\n",
      "|          1.12|              AT| PAY BY CELL|462-05360|2018-01-20 02:20:45|2018-01-20 01:50:45|  HAIGHT ST 500|\n",
      "+--------------+----------------+------------+---------+-------------------+-------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transactions.drop('_id').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "revenue_df = df_transactions.drop('_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+------------+---------+--------------------+--------------+--------------+---+------------+--------+\n",
      "|GROSS_PAID_AMT|METER_EVENT_TYPE|PAYMENT_TYPE|  POST_ID|        STREET_BLOCK|starttime_date|starttime_hour|dow|endtime_hour|timeDiff|\n",
      "+--------------+----------------+------------+---------+--------------------+--------------+--------------+---+------------+--------+\n",
      "|          0.35|              NS|        CASH|568-22390|     MISSION ST 2200|      2018-3-2|            10|  5|          10|     720|\n",
      "|           0.1|              NS|        CASH|680-17110|     TARAVAL ST 1700|     2018-2-26|            05|  1|          05|     160|\n",
      "|          1.25|              NS|  SMART CARD|360-05240|      CLEMENT ST 500|     2018-2-15|            09|  4|          09|    2000|\n",
      "|          3.75|              NS| CREDIT CARD|321-03100|       BALBOA ST 300|      2018-6-6|            11|  3|          12|    6750|\n",
      "|          1.12|              AT| PAY BY CELL|462-05360|       HAIGHT ST 500|     2018-1-20|            01|  6|          02|    1800|\n",
      "|           1.0|              NS| CREDIT CARD|593-05140|     OFARRELL ST 500|     2018-2-12|            03|  1|          03|    2429|\n",
      "|           1.0|              NS|        CASH|568-45030|     MISSION ST 4500|     2018-1-22|            01|  1|          01|    1600|\n",
      "|          0.35|              AT|        CASH|680-11080|     TARAVAL ST 1100|     2018-5-25|            02|  5|          03|     900|\n",
      "|           0.7|              AT|        CASH|591-12940|      OCEAN AVE 1200|      2018-3-9|            02|  5|          03|    1446|\n",
      "|           1.0|              NS| CREDIT CARD|614-19190|        POST ST 1900|     2018-2-20|            02|  2|          03|    1788|\n",
      "|           1.1|              NS|        CASH|907-00150|Mission and Norto...|     2018-3-21|            02|  3|          03|    1760|\n",
      "|          0.75|              NS| CREDIT CARD|350-34470|  CALIFORNIA ST 3400|     2018-1-16|            11|  2|          12|    1200|\n",
      "|           0.5|              NS|        CASH|109-12160|       09TH AVE 1200|     2018-5-25|            05|  5|          05|     720|\n",
      "|           8.5|              NS| CREDIT CARD|847-05160|    JEFFERSON ST 500|     2018-6-19|            01|  2|          03|    7200|\n",
      "|           6.0|              NS| CREDIT CARD|681-25170|      TAYLOR ST 2500|     2018-5-21|            12|  1|          02|  -36000|\n",
      "|          7.36|              NS| PAY BY CELL|418-01130|       FOLSOM ST 100|      2018-5-4|            02|  5|          04|    7200|\n",
      "|           0.5|              AT|        CASH|831-05140|EMBARCADERO SOUTH...|     2018-3-11|            05|  7|          08|    8355|\n",
      "|          0.25|              NS|        CASH|657-25240|  SAN BRUNO AVE 2500|     2018-1-16|            11|  2|          11|     400|\n",
      "|          0.25|              NS|        CASH|604-00070|        PERSIA AVE 0|     2018-6-21|            11|  4|          11|     400|\n",
      "|          3.04|              NS| CREDIT CARD|684-01840|     TOWNSEND ST 100|     2018-4-23|            02|  1|          03|    3353|\n",
      "+--------------+----------------+------------+---------+--------------------+--------------+--------------+---+------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import unix_timestamp, from_unixtime, date_format, col\n",
    "\n",
    "revenue_df = revenue_df.withColumn('starttime_date',date_format('SESSION_START_DT','yyyy-M-d'))\n",
    "revenue_df = revenue_df.withColumn('starttime_hour',date_format('SESSION_START_DT','HH').cast('string'))\n",
    "revenue_df = revenue_df.withColumn('dow',date_format('SESSION_START_DT','u').cast('string'))\n",
    "revenue_df = revenue_df.withColumn('endtime_hour',date_format('SESSION_END_DT','HH').cast('string'))\n",
    "revenue_df = revenue_df.withColumn('timeDiff', unix_timestamp('SESSION_END_DT')- unix_timestamp('SESSION_START_DT'))\n",
    "revenue_df.drop('SESSION_START_DT','SESSION_END_DT').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "wanted_df = revenue_df.groupBy(\"STREET_BLOCK\",\"dow\",\"starttime_hour\")\\\n",
    "                      .agg(avg('GROSS_PAID_AMT').alias('avgRevenue'), \n",
    "                      count('starttime_hour').alias('numNewTrans'),\n",
    "                      avg('timeDiff').alias('avgDur'),\n",
    "                      count(when(col('METER_EVENT_TYPE')=='NS', True)).alias('numNS'),\n",
    "                      count(when(col('METER_EVENT_TYPE')=='AT', True)).alias('numAT'),\n",
    "                      count(when(col('PAYMENT_TYPE')=='CASH', True)).alias('numCASH'),\n",
    "                      count(when(col('PAYMENT_TYPE')=='CREDIT CARD', True)).alias('numCC'),\n",
    "                      count(when(col('PAYMENT_TYPE')=='PAY BY CELL', True)).alias('numPhone'),\n",
    "                      count(when(col('PAYMENT_TYPE')=='SMART CARD', True)).alias('numSmartCard'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+--------------+------------------+-----------+-------------------+-----+-----+-------+-----+--------+------------+\n",
      "|        STREET_BLOCK|dow|starttime_hour|        avgRevenue|numNewTrans|             avgDur|numNS|numAT|numCASH|numCC|numPhone|numSmartCard|\n",
      "+--------------------+---+--------------+------------------+-----------+-------------------+-----+-----+-------+-----+--------+------------+\n",
      "|        BERRY ST 400|  1|            12|1.1257715430861723|        499|-24278.096192384768|  416|   83|    190|  196|      94|          19|\n",
      "|SOUTH VAN NESS AVE 0|  4|            03|1.9535009671179882|        517|  2503.413926499033|  416|  101|    203|  253|      44|          17|\n",
      "+--------------------+---+--------------+------------------+-----------+-------------------+-----+-----+-------+-----+--------+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wanted_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "wanted_df_derv = revenue_df.groupBy(\"STREET_BLOCK\",\"dow\",\"endtime_hour\").count()\n",
    "wanted_df_derv = wanted_df_derv.withColumnRenamed('STREET_BLOCK', 'STREET_BLOCK_derv')\n",
    "wanted_df_derv = wanted_df_derv.withColumnRenamed('dow', 'dow_derv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "wanted_df = wanted_df.join(wanted_df_derv, \n",
    "                           (wanted_df.STREET_BLOCK == wanted_df_derv.STREET_BLOCK_derv) &\n",
    "                           (wanted_df.dow == wanted_df_derv.dow_derv) &\n",
    "                           (wanted_df.starttime_hour == wanted_df_derv.endtime_hour)).drop('endtime_hour')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "wanted_df = wanted_df.withColumnRenamed('count','numEndTrans')\n",
    "wanted_df = wanted_df.withColumn('turnover', wanted_df['numNewTrans'] - wanted_df['numEndTrans'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---+--------------+-----------+-----+-----+-------+-----+--------+------------+-----------+--------+\n",
      "| STREET_BLOCK|dow|starttime_hour|numNewTrans|numNS|numAT|numCASH|numCC|numPhone|numSmartCard|numEndTrans|turnover|\n",
      "+-------------+---+--------------+-----------+-----+-----+-------+-----+--------+------------+-----------+--------+\n",
      "|    01ST ST 0|  4|            08|        193|  160|   33|     46|   60|      28|          59|        259|     -66|\n",
      "|  01ST ST 200|  3|            02|        281|  247|   34|     47|  145|      52|          37|        535|    -254|\n",
      "|  01ST ST 200|  4|            04|        277|  253|   24|     61|  192|      12|          12|        112|     165|\n",
      "|    02ND ST 0|  2|            08|        319|  315|    4|      3|  218|      75|          23|         62|     257|\n",
      "|  02ND ST 300|  6|            05|        500|  405|   95|    115|  294|      88|           3|        492|       8|\n",
      "|03RD AVE 1300|  4|            03|        233|  184|   49|     74|  127|      31|           1|        219|      14|\n",
      "|    03RD ST 0|  6|            12|         16|   16|    0|      2|   13|       1|           0|         12|       4|\n",
      "|  03RD ST 300|  3|            02|        599|  435|  164|    117|  314|     144|          24|        797|    -198|\n",
      "| 03RD ST 4600|  1|            02|        133|  123|   10|     69|   40|      17|           7|        124|       9|\n",
      "| 03RD ST 4800|  3|            01|         28|   26|    2|     15|    4|       9|           0|         30|      -2|\n",
      "| 04TH ST 1100|  6|            06|        596|  471|  125|    154|  371|      61|          10|        545|      51|\n",
      "| 04TH ST 1600|  2|            10|          8|    5|    3|      3|    4|       1|           0|         15|      -7|\n",
      "| 04TH ST 1700|  3|            08|        151|  125|   26|     43|   82|      26|           0|        159|      -8|\n",
      "|  04TH ST 300|  3|            03|        585|  500|   85|    173|  284|      80|          48|        492|      93|\n",
      "| 05TH AVE 200|  7|            05|          1|    1|    0|      1|    0|       0|           0|          1|       0|\n",
      "|  05TH ST 500|  5|            10|        967|  781|  186|    362|  450|     139|          16|       1069|    -102|\n",
      "|  05TH ST 500|  7|            06|          2|    2|    0|      2|    0|       0|           0|          2|       0|\n",
      "|  05TH ST 600|  4|            01|        263|  192|   71|     47|  138|      69|           9|        299|     -36|\n",
      "|  05TH ST 600|  7|            03|         22|   15|    7|      3|   11|       8|           0|         45|     -23|\n",
      "|    06TH ST 0|  1|            08|        159|  134|   25|     31|   70|      35|          23|         95|      64|\n",
      "+-------------+---+--------------+-----------+-----+-----+-------+-----+--------+------------+-----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wanted_df = wanted_df.drop('STREET_BLOCK_derv','dow_derv')\n",
    "wanted_df.drop('block','avgRevenue','avgDur').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_schedules = df_schedules.withColumnRenamed(\"Post ID\", \"POST_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_schedules.columns:\n",
    "    df_schedules = df_schedules.withColumnRenamed(col, \"_\".join(col.split()).upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "u'Table `Transactions` already exists.;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-ddfe43389297>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_transactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'STREET_BLOCK'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'POST_ID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistinct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Transactions\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msaveAsTable\u001b[0;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    773\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 775\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: u'Table `Transactions` already exists.;'"
     ]
    }
   ],
   "source": [
    "df_transactions.select(['STREET_BLOCK', 'POST_ID']).distinct().write.saveAsTable(\"Transactions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meters.write.saveAsTable(\"Meters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_schedules.write.saveAsTable(\"Schedules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sched_base = ss.sql(\"\"\"\n",
    "SELECT POST_ID, AVG_RATE AS RATE, HAS_OVERRIDE FROM (\n",
    "\n",
    "    SELECT POST_ID, AVG_RATE, CASE WHEN num >= 2 THEN 1 ELSE 0 END HAS_OVERRIDE FROM\n",
    "    (\n",
    "        SELECT *,\n",
    "        count(1) OVER(\n",
    "        PARTITION BY POST_ID\n",
    "        ) num,\n",
    "        AVG(RATE) OVER(\n",
    "        PARTITION BY POST_ID\n",
    "        ) AVG_RATE\n",
    "\n",
    "        FROM Schedules\n",
    "    )    \n",
    ")\n",
    "GROUP BY 1, 2, 3\n",
    "\"\"\")\n",
    "sched_base.write.saveAsTable('Schedules_Base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meter_dummy = ss.sql(\"\"\"\n",
    "SELECT POST_ID,\n",
    "RATE,\n",
    "HAS_OVERRIDE,\n",
    "CASE WHEN CAP_COLOR = 'Green' THEN 1 ELSE 0 END GREEN,\n",
    "CASE WHEN CAP_COLOR = 'Grey' THEN 1 ELSE 0 END GREY\n",
    "FROM\n",
    "Schedules_Base\n",
    "join\n",
    "(\n",
    "    SELECT * FROM Meters\n",
    "    WHERE (Meters.CAP_COLOR = 'Green' OR Meters.CAP_COLOR = 'Grey')\n",
    ")\n",
    "USING(POST_ID)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_post = ss.sql(\"\"\"\n",
    "    SELECT STREET_BLOCK, POST_ID\n",
    "    FROM Transactions\n",
    "    GROUP BY 1,2\n",
    "    ORDER BY 1\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_post_full = block_post.join(meter_dummy, how='inner', on='POST_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_post_full.write.saveAsTable(\"Post_block\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_info = ss.sql(\"\"\"\n",
    "SELECT STREET_BLOCK,\n",
    "MAX(RATE) RATE,\n",
    "SUM(HAS_OVERRIDE) NUM_OR,\n",
    "SUM(GREEN) NUM_GREEN,\n",
    "SUM(GREY) NUM_GREY\n",
    "FROM Post_block\n",
    "GROUP BY 1\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_info.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "va = VectorAssembler(outputCol=\"features\", inputCols=block_info.columns[1:]) #except the last col.\n",
    "block_feat = va.transform(block_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "kmeans =  KMeans(k = 90, maxIter = 200, tol = 0.1) # k = 10 as there are 10 different handwritten numbers.\n",
    "model = kmeans.fit(block_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_df = model.transform(block_feat).select('STREET_BLOCK', 'RATE', 'NUM_OR', 'NUM_GREEN', 'NUM_GREY', 'PREDICTION')\\\n",
    "    .withColumnRenamed('PREDICTION', 'CLUSTER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = wanted_df.join(kmeans_df, \"STREET_BLOCK\", \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalal_df = final_df.drop(\"STREET_BLOCK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalal_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalal_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "def indexStringColumns(df, cols):\n",
    "    #variable newdf will be updated several times\n",
    "    newdf = df\n",
    "    \n",
    "    for c in cols:\n",
    "        #For each given colum, fits StringIndexerModel.\n",
    "        si = StringIndexer(inputCol=c, outputCol=c+\"-num\",stringOrderType=\"alphabetDesc\")\n",
    "        sm = si.fit(newdf)\n",
    "        #Creates a DataFame by putting the transformed values in the new colum with suffix \"-num\" \n",
    "        #and then drops the original columns.\n",
    "        #and drop the \"-num\" suffix. \n",
    "        newdf = sm.transform(newdf).drop(c)\n",
    "        newdf = newdf.withColumnRenamed(c+\"-num\", c)\n",
    "    return newdf\n",
    "\n",
    "stringindex_anotherdf = indexStringColumns(finalal_df, ['CLUSTER','dow','starttime_hour'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "def oneHotEncodeColumns(df, cols):\n",
    "    newdf = df\n",
    "    for c in cols:\n",
    "        #For each given colum, create OneHotEncoder. \n",
    "        #dropLast : Whether to drop the last category in the encoded vector (default: true)\n",
    "        onehotenc = OneHotEncoder(inputCol=c, outputCol=c+\"-onehot\", dropLast=False)\n",
    "        #Creates a DataFame by putting the transformed values in the new colum with suffix \"-onehot\" \n",
    "        #and then drops the original columns.\n",
    "        #and drop the \"-onehot\" suffix. \n",
    "        newdf = onehotenc.transform(newdf).drop(c)\n",
    "        newdf = newdf.withColumnRenamed(c+\"-onehot\", c)\n",
    "    return newdf\n",
    "\n",
    "onehot_anotherdf = oneHotEncodeColumns(stringindex_anotherdf, ['CLUSTER','dow','starttime_hour'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_anotherdf.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_anotherdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the data with Vector Assembler.\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"dow\",\"starttime_hour\",\"avgRevenue\",\"numNewTrans\",\"avgDur\",\"numNS\",\"numAT\",\"numCASH\",\"numCC\",\"numPhone\",\"numSmartCard\",\"numEndTrans\",\"RATE\",\"NUM_OR\",\"NUM_GREEN\",\"NUM_GREY\",\"CLUSTER\"],\n",
    "    outputCol=\"features\")\n",
    "\n",
    "output = assembler.transform(onehot_anotherdf)\n",
    "\n",
    "# va = VectorAssembler(outputCol=\"features\", inputCols=anotherdf.columns[1:]) #except the last col.\n",
    "penlpoints = output.select(\"features\", \"turnover\")\n",
    "penlpoints.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = penlpoints.randomSplit([0.8, 0.2])\n",
    "\n",
    "#cache() : the algorithm is interative and training and data sets are going to be reused many times.\n",
    "train = splits[0].cache()\n",
    "test = splits[1].cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the data.\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "# Paramenters\n",
    "#maxDepth : maximum tree depth (default : 5).\n",
    "#maxBins : maximum number of bins when binning continuous features (default : 32).\n",
    "#minInstancesPerNode : minimum number of dataset samples each branch needs to have after a split (default : 1).\n",
    "#minInfoGain : minimum information gain for a split (default : 0).\n",
    "dt = DecisionTreeRegressor(labelCol=\"turnover\")\n",
    "dtmodel = dt.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test data.\n",
    "dtpredicts = dtmodel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator = RegressionEvaluator(labelCol=\"turnover\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(dtpredicts)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
